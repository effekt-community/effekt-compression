import src/utils/circular_array
import stream
import io
import io/error
import src/utils/stream_io
import bytearray

/// LZSS block structure:
/// 
/// +----------------------------------------------+--------------------+----------------------------------+
/// | 4 bytes - token count                        | Encoded content    | 3 bits - how many bits to ignore |
/// |                                              |                    | before next block                |
/// |                                              |                    |                                  |
/// |                                              |                    |                                  |
/// +----------------------------------------------+--------------------+----------------------------------+
///
/// Numbers are in little endian - less significant bytes/bits first
/// 
/// The encoded content consists of tokens. Each token is either a single byte or a pair of offset and length.
/// To encode a token, we use a flag bit to distinguish between the two cases. If the flag bit is 0, it is a single byte, 
/// otherwise it is a pair of offset and length.
/// 
/// The offset is the starting position of a match. It is the number of bytes one needs to go back in the search buffer.
/// The length is the number of bytes to copy from the search buffer.

def chunkReader(): (Int, ByteArray) / read[Byte] = {
    val chunkSize = 2 * 4096
    val buffer = bytearray::allocate(chunkSize)
    var offset = 0

    def go(): Unit = try {
        val readByte = do read[Byte]()

        buffer.set(offset, readByte)
        offset = offset + 1

        // read only until we have space in the buffer
        if (offset < chunkSize) {
            go()
        } else {
            ()
        }
    } with stop {
        ()
    }

    go()
    (offset, buffer)
}

/// Find the longest match in the look-ahead buffer for the given processed byte.
/// Returns offset and length of the match.
def find_longest_match(processedByte: Byte, searchBuffer: CircularArray[Byte], lookAheadBuffer: CircularArray[Byte]) = {
    // keeps track of the back-offset in the search buffer
    var offsetCounter = searchBuffer.elementCount.get()
    var maxMatchLength = 0
    var maxMatchOffset = 0

    searchBuffer.foreachIndex { (i, searchBufferElement) =>

        // find the longest matching byte sequence starting at the index i
        if (searchBufferElement.toInt == processedByte.toInt) {
            var matchLength = 1

            var lookAheadBufferIdx = 0
            var searchBufferIdx = i + 1

            loop { {l} =>
                // stop if the match length is 18 (maximum we can store in 4 bits)
                if (lookAheadBufferIdx >= lookAheadBuffer.elementCount.get() || searchBufferIdx >= searchBuffer.elementCount.get() || matchLength == 18) {
                    l.break()
                }

                val searchBufferMatchElement = searchBuffer.get(searchBufferIdx)
                val lookAheadBufferElement = lookAheadBuffer.get(lookAheadBufferIdx)

                if (searchBufferMatchElement.toInt == lookAheadBufferElement.toInt) {
                    matchLength = matchLength + 1
                } else {
                    l.break()
                }

                lookAheadBufferIdx = lookAheadBufferIdx + 1
                searchBufferIdx = searchBufferIdx + 1
            }


            if (matchLength > maxMatchLength) {
                maxMatchLength = matchLength
                maxMatchOffset = offsetCounter
            }
        }

        offsetCounter = offsetCounter - 1
    }

    if (maxMatchLength < 3) {
        (0, 0)
    } else {
        (maxMatchOffset, maxMatchLength)
    }
}

/// Fill the look-ahead buffer with the first 18 bytes from the data chunk.
def init_look_ahead_buffer(lookAheadBuffer: CircularArray[Byte], buffer: ByteArray, byteCount: Int): Int = {
    val lookAheadBufferSize = lookAheadBuffer.rawContent.size

    var bufferIndex = 1

    each(1, lookAheadBufferSize + 1) { (i) {loop} =>
        if (i >= byteCount) {
            loop.break()
        }

        val byte = buffer.get(i)
        lookAheadBuffer.append(byte)
        bufferIndex = bufferIndex + 1
    }

    bufferIndex
}

/// Count the number of tokens in the data chunk, to be able to calculate the bit offset before next chunk.
def count_tokens(buffer: ByteArray, byteCount: Int): Int = {
    var tokenCount = 0

    val searchBufferSize = 4096
    val lookAheadBufferSize = 18

    val searchBuffer = circular_array[Byte](searchBufferSize)
    val lookAheadBuffer = circular_array[Byte](lookAheadBufferSize)

    var processedByte = buffer.get(0)
    var bufferIndex = init_look_ahead_buffer(lookAheadBuffer, buffer, byteCount)

    loop { {l} =>
        with on[OutOfBounds].report
        val (matchOffset, matchLength) = find_longest_match(processedByte, searchBuffer, lookAheadBuffer)

        searchBuffer.append(processedByte)

        tokenCount = tokenCount + 1

        if(not(matchLength == 0)) {
            // matchLength - 1 because of the processed byte
            repeat (matchLength - 1) {
                val value = lookAheadBuffer.popFront()
                value match {
                    case Some(v) => searchBuffer.append(v)
                    case None() => panic("Fatal error, look-ahead buffer should contain matched bytes")
                }
            }

            // fill up the look-ahead buffer with new bytes
            repeat (matchLength - 1) { {loop} =>
                if (bufferIndex >= byteCount) {
                    loop.break()
                }
                lookAheadBuffer.append(buffer.get(bufferIndex))
                bufferIndex = bufferIndex + 1
            }

        }

        // break the loop if there are no new characters to read
        val value = lookAheadBuffer.popFront()
        value match {
            case Some(v) => processedByte = v
            case None() => l.break()
        }

        // fill one byte to the lookAheadBuffer for the popped 
        if (bufferIndex < byteCount) {
            lookAheadBuffer.append(buffer.get(bufferIndex))
            bufferIndex = bufferIndex + 1
        }
    
    }

    tokenCount
}

/// Compress the data chunk (block) into the output stream.
/// Count the tokens first and then compress the data chunk.
def writeCompressedBlockStream(buffer: ByteArray, byteCount: Int): Unit / { emit[Byte], read[Byte]} = {
    with streamWriter
    with on[OutOfBounds].report

    println("Starting LZSS block compression...")
    
    val searchBufferSize = 4096
    val lookAheadBufferSize = 18

    val searchBuffer = circular_array[Byte](searchBufferSize)
    val lookAheadBuffer = circular_array[Byte](lookAheadBufferSize)

    val tokenCount = count_tokens(buffer, byteCount)
    println("Token count: " ++ tokenCount.show)

    var processedByte = buffer.get(0)
    var bufferIndex = init_look_ahead_buffer(lookAheadBuffer, buffer, byteCount)
    integerToBytesStream(tokenCount)

    loop { {l} =>
        val (matchOffset, matchLength) = find_longest_match(processedByte, searchBuffer, lookAheadBuffer)

        if (matchLength == 0) {
            // we will emit the byte itself with a 0 flag
            do writeBit(false)
            do writeByte(processedByte)
        } else {
            // emit the offset and length of the match with a leading 1 flag
            do writeBit(true)

            // we need to subtract 1 from the offset because offset indexing starts at 1 (or at element count of search buffer)
            // In case of offset being 4096, it wouldnt fit into 12 bits
            numberToBitsStream(matchOffset - 1, 12)
            numberToBitsStream(matchLength - 3, 4)
        }

        searchBuffer.append(processedByte)

        if(not(matchLength == 0)) {
            // matchLength - 1 because of the processed byte
            repeat (matchLength - 1) {
                val value = lookAheadBuffer.popFront()
                value match {
                    case Some(v) => searchBuffer.append(v)
                    case None() => panic("Fatal error, look-ahead buffer should contain matched bytes")
                }
            }

            // fill up the look-ahead buffer with new bytes
            repeat (matchLength - 1) { {loop} =>
                if (bufferIndex >= byteCount) {
                    loop.break()
                }
                lookAheadBuffer.append(buffer.get(bufferIndex))
                bufferIndex = bufferIndex + 1
            }
        }

        // break the loop if there are no new characters to read
        val value = lookAheadBuffer.popFront()
        value match {
            case Some(v) => processedByte = v
            case None() => l.break()
        }

        // fill one byte to the lookAheadBuffer for the popped one
        if (bufferIndex < byteCount) {
            lookAheadBuffer.append(buffer.get(bufferIndex))
            bufferIndex = bufferIndex + 1
        }
    }

    // account for 3 bits occupied by the stored value of bitPadding
    val bitPadding = mod(8 - mod(tokenCount + 3, 8), 8) 
    println("Bit padding: " ++ bitPadding.show)
    numberToBitsStream(bitPadding, 3)


    println("LZSS block compression was successful")
}

/// Entry point for compressing a data stream.
/// Reads the stream chunk by chunk and runs LZSS on it.
def compressReaderIntoStream() = {
    println("Starting LZSS compression...")
    loop { {l} =>
        val (byteCount, buffer) = chunkReader()
        if (byteCount == 0) {
            // no more data to read => we are done
            println("No more data to read, terminating compression")
            l.break()
        } else {
            writeCompressedBlockStream(buffer, byteCount)
        }
    }



}

/// ********************* \\\
///     DECOMPRESSION     \\\
/// ********************* \\\

def stopHandler() {prog: => Unit / stop} = {
    try {
        prog()
    } with stop {
        println("Decompression was successful")
    }
}

/// Decompress a single LZSS block.
/// Read token count first, then read token by token and decompress.
/// Read number of bits to ignore at the end and skip them
def decompressBlock() = {
    with streamReader
    with streamWriter
    with on[OutOfBounds].report

    println("Starting LZSS block decompression...")

    val searchBufferSize = 4096
    val searchBuffer = circular_array[Byte](searchBufferSize)

    val tokenCount = bytesToIntegerReader()
    println("Token count: " ++ tokenCount.show)

    repeat(tokenCount) { {l} =>

        var flag = do readBit()
        if (flag) {
            var offset = bitsToNumberReader(12) + 1
            var length = bitsToNumberReader(4) + 3

            val start = searchBuffer.elementCount.get() - offset
            val end = start + length

            each (start, end) { i =>
                val byte = searchBuffer.get(i)

                do writeByte(byte)
                searchBuffer.append(byte)
            }

        } else {
            val byte = do readByte()
            do writeByte(byte)
            searchBuffer.append(byte)
        }
    }

    val bitsToIgnore = bitsToNumberReader(3)
    // skip the padding bits
    repeat(bitsToIgnore) {
        val _ = do readBit()
        ()
    }

    println("LZSS block decompression was successful")
}

/// Entry point for decompressing a data stream.
/// Reads the stream chunk by chunk. Restores the data back according to the search buffer based on the read tokens.
def decompressReaderIntoStream() = {
    println("Starting LZSS decompression...")

    loop { {l} =>
        try {
            decompressBlock()
        } with stop {
            println("No more data to read, terminating decompression")
            println("LZSS block decompression successful")
            l.break()
        }
    }
}